# Large Language Models - Homework 1

This homework is part of the **Large Language Model** course and explores different NLP model architectures, including N-Gram, RNN, and Transformer models, using real-world datasets.

---

## ğŸ“š Table of Contents

- [ğŸ“ Dataset Description](#-dataset-description)
- [ğŸ§  Part 1: N-Gram Model](#-part-1-n-gram-model)
- [ğŸ” Part 2: RNN Model](#-part-2-rnn-model)
- [ğŸ§² Part 3: Self-Attention Layer in Transformer](#-part-3-self-attention-layer-in-transformer)
- [ğŸ“Š Comparison and Analysis](#-comparison-and-analysis)
- [âš™ï¸ Environment & Tools](#ï¸-environment--tools)
- [ğŸš« Note](#-note)

---

## ğŸ“ Dataset Description

- **Part 1 & 2**: Recipe step sentences from `Food.com Recipes and Interactions`  
  - `train.txt`: ~2.6M entries  
  - `test.txt`: ~650K entries  
  - `incomplete.txt`: 10 incomplete sentences

- **Part 3**: Emotion classification dataset  
  - `train.csv`, `val.csv`, `test.csv`  
  - Each line contains a sentence and its corresponding emotion tag (5 total emotions)

---

## ğŸ§  Part 1: N-Gram Model

Implemented bigram (n=2) and trigram (n=3) models from scratch using Python's `defaultdict`.

- âœ… **Test Accuracy**: ~24% to 25%, within the expected threshold.
- âœï¸ **Sentence Completion**: Used the trigram model to extend each incomplete sentence to 20 words.

---

## ğŸ” Part 2: RNN Model

Built an RNN using **PyTorch** with the following hyperparameters:

- `hidden_size = 128`  
- `num_layers = 2`  
- `learning_rate = 0.001`  
- `epochs = 10`  
- `batch_size = 32`  

### Results:
- ğŸ“‰ Plotted training loss and accuracy curves.
- ğŸ§ª Evaluated model accuracy on `test.txt`.
- âœï¸ Used the model to complete the same 10 incomplete sentences from Part 1 and compared the results with the N-gram model.

---

## ğŸ§² Part 3: Self-Attention Layer in Transformer

This part focuses on implementing the core components of a Transformer model for emotion classification.

- ğŸ§© Completed the **Multi-head Attention** and **Transformer Encoder Layer** from scratch.
- ğŸ“ˆ Trained the model on the provided dataset. It achieved the expected accuracy threshold.
- âš™ï¸ Experimented with different **numbers of attention heads** (1, 4, 8) to observe performance variation.
  - The number of heads significantly impacted the modelâ€™s performance and training behavior.

---

## ğŸ“Š Comparison and Analysis

Compared the models based on:

- âœ… Accuracy  
- â±ï¸ Computation time  
- ğŸ§  Memory and hardware usage  
- âœï¸ Quality of sentence completion (N-Gram vs. RNN)  
- ğŸ§² Effect of attention heads on Transformer model performance

---

## âš™ï¸ Environment & Tools

- Python 3.x  
- PyTorch  
- `NumPy`, `Pandas`, `collections`, etc.  
- GPU support for RNN and Transformer training  
- No use of advanced NLP libraries for restricted parts (e.g., no `torch.nn.MultiheadAttention`)

---

## ğŸš« Note

All implementations were done independently following assignment guidelines. No plagiarism or use of restricted libraries.
